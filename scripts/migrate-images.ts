import { createClient } from "@supabase/supabase-js";
import sharp from "sharp";
import { readFileSync } from "fs";
import { resolve } from "path";

const url = process.env.SUPABASE_URL!;
const service = process.env.SUPABASE_SERVICE_ROLE_KEY!;
const supa = createClient(url, service);

// WebP quality and responsive variants configuration
const WEBP_QUALITY = 80;
const VARIANTS = [
  { width: 1200, height: 675 }, // 16:9
  { width: 960, height: 540 },
  { width: 640, height: 360 },
  { width: 320, height: 180 }
];

interface ImageVariant {
  width: number;
  url: string;
}

async function fetchAsArrayBuffer(remoteUrl: string): Promise<ArrayBuffer> {
  try {
    const res = await fetch(remoteUrl);
    if (!res.ok) throw new Error(`Fetch failed: ${remoteUrl} - ${res.status}`);
    return await res.arrayBuffer();
  } catch (error) {
    console.error(`Failed to fetch ${remoteUrl}:`, error);
    throw error;
  }
}

function generateFilePath(kind: "projects" | "blog", slug: string, basename: string, width: number): string {
  return `media/${kind}/${slug}/${basename}-${width}.webp`.toLowerCase();
}

function generateSrcSet(baseUrl: string, basename: string, variants: ImageVariant[]): string {
  return variants
    .map(variant => `${baseUrl.replace('-1200.webp', `-${variant.width}.webp`)} ${variant.width}w`)
    .join(', ');
}

async function processImageToWebP(buffer: ArrayBuffer, width: number, height: number): Promise<Buffer> {
  return await sharp(Buffer.from(buffer))
    .resize(width, height, { fit: 'cover', position: 'center' })
    .webp({ quality: WEBP_QUALITY })
    .toBuffer();
}

async function uploadImageVariants(
  kind: "projects" | "blog", 
  slug: string, 
  srcUrl: string, 
  basename: string = "image"
): Promise<{ largestUrl: string; srcset: string; variants: ImageVariant[] }> {
  
  console.log(`Processing ${srcUrl} for ${kind}/${slug}...`);
  
  // Fetch original image
  const originalBuffer = await fetchAsArrayBuffer(srcUrl);
  const variants: ImageVariant[] = [];
  
  // Process each variant
  for (const { width, height } of VARIANTS) {
    const filename = `${basename}-${width}.webp`;
    const path = generateFilePath(kind, slug, basename, width);
    
    // Check if already exists
    const { data: exists, error: listError } = await supa.storage
      .from("media")
      .list(`${kind}/${slug}`, { search: filename });
    
    if (listError) {
      console.warn(`Error checking existing files: ${listError.message}`);
    }

    let publicUrl: string;
    
    if ((exists || []).some(f => f.name === filename)) {
      console.log(`  ‚è≠Ô∏è  File exists: ${path}`);
      publicUrl = supa.storage.from("media").getPublicUrl(path).data.publicUrl;
    } else {
      // Process and upload
      console.log(`  üîÑ Converting to WebP ${width}x${height}...`);
      const webpBuffer = await processImageToWebP(originalBuffer, width, height);
      
      const { error: upErr } = await supa.storage.from("media").upload(path, webpBuffer, {
        cacheControl: "31536000", // 1 year cache
        upsert: false,
        contentType: "image/webp"
      });
      
      if (upErr) {
        console.error(`Upload error for ${path}:`, upErr);
        throw upErr;
      }
      
      publicUrl = supa.storage.from("media").getPublicUrl(path).data.publicUrl;
      console.log(`  ‚úÖ Uploaded: ${filename}`);
    }
    
    variants.push({ width, url: publicUrl });
  }
  
  // Return largest (1200w) as primary URL
  const largestUrl = variants[0].url; // First variant is 1200w
  const srcset = generateSrcSet(largestUrl, basename, variants);
  
  return { largestUrl, srcset, variants };
}

async function migrateProjectImages() {
  console.log("üîÑ Migrating project images to WebP with responsive variants...");
  
  const { data: projects, error: pErr } = await supa
    .from("projects")
    .select(`
      id, 
      slug, 
      project_images(
        id,
        url,
        sort_order,
        alt
      )
    `);
    
  if (pErr) {
    console.error("Error fetching projects:", pErr);
    throw pErr;
  }

  for (const project of projects || []) {
    console.log(`\nüìÅ Processing project: ${project.slug}`);
    
    for (const img of (project as any).project_images || []) {
      // Skip if already migrated (contains webp and storage URL)
      if (!img.url || (/storage\/v1\/object\/public\/media/i.test(img.url) && img.url.includes('.webp'))) {
        console.log(`  ‚è≠Ô∏è  Already migrated: ${img.url}`);
        continue;
      }

      try {
        const basename = `image-${img.sort_order || 1}`;
        const { largestUrl, srcset } = await uploadImageVariants("projects", project.slug, img.url, basename);
        
        // Update database with new URL and srcset info
        const { error: uErr } = await supa
          .from("project_images")
          .update({ 
            url: largestUrl,
            // Store srcset in alt field temporarily, or add new column
            alt: img.alt || `Project image ${img.sort_order || 1}`
          })
          .eq("id", img.id);
          
        if (uErr) {
          console.error(`Error updating image record ${img.id}:`, uErr);
          throw uErr;
        }
        
        console.log(`  ‚úÖ Updated image ${img.id} with WebP variants`);
      } catch (error) {
        console.error(`  ‚ùå Failed to migrate image ${img.id}:`, error);
        // Continue with next image
      }
    }
  }
}

async function migrateBlogImages() {
  console.log("\nüîÑ Migrating blog images...");
  
  // Check if blog_posts table has hero_url or similar field
  const { data: blogPosts, error: bErr } = await supa
    .from("blog_posts")
    .select("id, slug, body")
    .eq("status", "published");
    
  if (bErr) {
    console.error("Error fetching blog posts:", bErr);
    return;
  }

  for (const post of blogPosts || []) {
    console.log(`\nüìù Processing blog post: ${post.slug}`);
    
    // Extract images from body content if needed
    if (post.body && typeof post.body === 'object') {
      // This would depend on your specific blog body structure
      // For now, we'll skip blog migration unless you have hero images
      console.log(`  ‚ÑπÔ∏è  Blog image migration not implemented yet`);
    }
  }
}

async function ensureMediaBucket() {
  console.log("üì¶ Checking media bucket...");
  
  const { data: buckets, error: bucketError } = await supa.storage.listBuckets();
  if (bucketError) throw bucketError;
  
  const mediaBucket = buckets?.find(b => b.id === 'media');
  if (!mediaBucket) {
    console.log("üì¶ Creating media bucket...");
    const { error: createError } = await supa.storage.createBucket('media', { 
      public: true,
      allowedMimeTypes: ['image/webp', 'image/jpeg', 'image/png'],
      fileSizeLimit: 10485760 // 10MB
    });
    if (createError) throw createError;
    console.log("‚úÖ Media bucket created");
  } else {
    console.log("‚úÖ Media bucket exists");
  }
}

async function main() {
  try {
    console.log("üöÄ Starting Phase 5E: Image Migration to WebP + Responsive");
    
    await ensureMediaBucket();
    await migrateProjectImages();
    await migrateBlogImages();
    
    console.log("\n‚úÖ Phase 5E migration completed successfully!");
    console.log("üìä All images converted to WebP with responsive variants (320w, 640w, 960w, 1200w)");
    console.log("üéØ Use srcset in components for optimal loading");
    
  } catch (error) {
    console.error("\n‚ùå Migration failed:", error);
    process.exit(1);
  }
}

// Only run if called directly
if (require.main === module) {
  main();
}

export { main as migrateImages };